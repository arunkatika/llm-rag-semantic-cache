# ğŸ§  LLM RAG (Retrieval-Augmented Generation) App with Semantic Caching

A fully **offline**, **privacy-first** Retrieval-Augmented Generation (RAG) application that allows users to upload **PDFs** and query them using **LLMs (LLaMA 3.2B)** with **semantic caching** and **Re-ranking** â€” all running **locally** on your machine.

> ğŸ” No cloud. No API keys. 100% local inference & vector search.

---
![Thumbnail](https://github.com/user-attachments/assets/0af67786-9871-4fcb-84ba-513f9899ab9c)

## ğŸ“Œ Features

- ğŸ” Document Q&A using semantic search over uploaded PDFs
- ğŸ§  LLM Integration with [Ollama](https://ollama.com) (LLaMA 3.2B)
- ğŸ§­ Vector indexing and chunking using **ChromaDB**
- ğŸ§  Semantic Caching with **Redis** to avoid redundant queries
- ğŸª„ Re-ranking using **CrossEncoder (MS MARCO)**
- ğŸ’¡ Clean UI built with **Streamlit**
- ğŸ“¦ Fully local architecture (no external API calls)

---

## âš™ï¸ Tech Stack

| Tool                | Role                                  |
|---------------------|----------------------------------------|
| ğŸ¦™ Ollama            | Local LLM & Embedding model serving    |
| ğŸ§  LLaMA 3.2B         | Large Language Model via Ollama        |
| ğŸ“š Nomic Embed Text  | Embedding model for vector search      |
| ğŸ“¦ ChromaDB          | Vector store for document chunks       |
| ğŸ” Redis + LangChain | Semantic cache storage & retrieval     |
| ğŸ“Š CrossEncoder      | Re-ranking of retrieved results        |
| ğŸ“„ Streamlit         | UI for document upload and Q&A         |

---

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/arunkatika/llm-rag-semantic-cache.git
cd llm-rag-semantic-cache
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Pull the models using Ollama
```bash
ollama pull llama3:3b
ollama pull nomic-embed-text
```

### 4. Run Redis using Docker
```bash
docker run -d -p 6379:6379 --name redis redis/redis-stack:latest
```

### 5. Start the app
```bash
streamlit run app.py
```

---

## ğŸ“ How It Works

1. **Upload PDF** â†’ Document is chunked and embedded â†’ Stored in **ChromaDB**
2. **Ask a question**:
   - First checks **Redis cache** for semantic match
   - If **match found**: returns cached answer instantly âš¡
   - If **no match**: 
     - Queries vector store
     - Re-ranks with CrossEncoder
     - Passes top results to LLaMA 3.2B
     - Caches the new Q&A pair in Redis

---

## ğŸ§ª Use Cases

- Internal document search assistant
- Study notes summarizer
- Legal/compliance document QA
- Resume & job document chatbot
- Enterprise knowledge Q&A system

---

## ğŸ›  Example: CSV Format for Semantic Cache

```csv
question,answer
"What is the target candidate description?","The AWS Certified AI Practitioner is intended for..."
"What is AWS Certified AI Practitioner?","It validates foundational knowledge of AI/ML services on AWS..."
```

ğŸ“‚ Upload this using **Cache mode** in the app.

---

## ğŸ§  Tips

- Use meaningful filenames â€” they become embedding IDs
- Monitor Redis & Chroma for performance/debugging
- Tune semantic cache threshold for match sensitivity

---

## ğŸ”§ Development Notes

### ğŸ“¦ Install core dependencies
```bash
pip install ollama chromadb sentence-transformers streamlit pymupdf langchain-community
```

### ğŸ“¦ For semantic cache support
```bash
pip install langchain-ollama==0.2.2 langchain-redis==0.2.0
```

### ğŸ³ Redis via Docker
```bash
docker run -d -p 6379:6379 redis/redis-stack:latest
```

### ğŸ§ª Redis Cache Debugging
```bash
docker exec -it redis bash
redis-cli
KEYS *
```

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss your ideas.

---
